# !/usr/bin/python
# -*- coding: utf-8 -*-
# @time    : 2023/3/25 21:56
# @author  : Mo
# @function: æ¨ç†


import random
import time
import sys
import os
path_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
print(path_root)
sys.path.append(path_root)
from qwen_sft.ft_qwen.config import CUDA_VISIBLE_DEVICES, USE_TORCH, CPU_NUMS  # from config
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:3072"
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # CUDA_VISIBLE_DEVICES
os.environ["USE_TORCH"] = USE_TORCH
os.environ["OMP_NUM_THREADS"] = CPU_NUMS  # export OMP_NUM_THREADS=1
os.environ["OPENBLAS_NUM_THREADS"] = CPU_NUMS  # export OPENBLAS_NUM_THREADS=1
os.environ["MKL_NUM_THREADS"] = CPU_NUMS  # export MKL_NUM_THREADS=1
os.environ["VECLIB_MAXIMUM_THREADS"] = CPU_NUMS  # export VECLIB_MAXIMUM_THREADS=1
os.environ["NUMEXPR_NUM_THREADS"] = CPU_NUMS  # export NUMEXPR_NUM_THREADS=1

from peft import (get_peft_model, LoraConfig)
from transformers import GenerationConfig
import torch

# from transformers import LlamaForCausalLM, LlamaModel
# from transformers import LlamaTokenizer, LlamaConfig
from qwen_sft.models.qwen.modeling_qwen import QWenLMHeadModel as LLMModel
from qwen_sft.models.qwen.tokenization_qwen import QWenTokenizer as LLMTokenizer
from qwen_sft.models.qwen.configuration_qwen import QWenConfig as LLMConfig
from qwen_sft.ft_qwen.config import PATH_MODEL_PRETRAIN, DATA_PATH, MODEL_SAVE_DIR, REPO_ID
from qwen_sft.ft_qwen.config import MICRO_BATCH_SIZE, BATCH_SIZE, GRADIENT_ACCUMULATION_STEPS
from qwen_sft.ft_qwen.config import LEARNING_RATE, EPOCHS, SAVE_STEPS, VAL_SET_SIZE, TARGET_MODULES
from qwen_sft.ft_qwen.config import IS_PARALLELIZABLE, MODEL_PARALLEL, USE_CACHE
from qwen_sft.ft_qwen.config import MAX_LENGTH_Q, MAX_LENGTH_A, MAX_LENGTH_QA
from qwen_sft.ft_qwen.config import LORA_DROPOUT, LORA_ALPHA, LORA_R
from qwen_sft.ft_qwen.config import USE_CUDA


# device_map = {"": int(os.environ.get("LOCAL_RANK") or 0)}
world_size = int(os.environ.get("WORLD_SIZE", 1))
ddp = world_size != 1
device_map = "auto"
# USE_CUDA = True
print(device_map)
print(ddp)


def load_model_state(model, model_save_dir="./", model_name="pytorch_model.bin", device="cpu"):
    """  ä»…åŠ è½½æ¨¡å‹å‚æ•°(æ¨èä½¿ç”¨)  """
    try:
        path_model = os.path.join(model_save_dir, model_name)
        peft_config = LoraConfig.from_pretrained(model_save_dir)
        peft_config.inference_mode = True
        model = get_peft_model(model, peft_config)
        state_dict = torch.load(path_model, map_location=torch.device(device))
        # print(state_dict.keys())
        state_dict = {"base_model.model." + k.replace("_orig_mod.", "")
                      .replace(".lora_A.weight", ".lora_A.default.weight")
                      .replace(".lora_B.weight", ".lora_B.default.weight")
                      : v for k, v in state_dict.items()}
        print(state_dict.keys())
        print("#" * 128)
        ### æ’æŸ¥ä¸å­˜åœ¨model.keysçš„ state_dict.key
        name_dict = {name: 0 for name, param in model.named_parameters()}
        print(name_dict.keys())
        print("#" * 128)
        for state_dict_key in state_dict.keys():
            if state_dict_key not in name_dict:
                print("{} is not exist!".format(state_dict_key))
        model.load_state_dict(state_dict, strict=False)
        # model.to(device)
        print("******model loaded success******")
        print("self.device: {}".format(device))
    except Exception as e:
        print(str(e))
        raise Exception("******load model error******")
    return model
def save_model_state(model, config=None, model_save_dir="./", model_name="pytorch_model.bin"):
    """  ä»…ä¿å­˜æ¨¡å‹å‚æ•°(æ¨èä½¿ç”¨)  """
    if not os.path.exists(model_save_dir):
        os.makedirs(model_save_dir)
    if config:
        config.save_pretrained(model_save_dir)
    # save model
    path_model = os.path.join(model_save_dir, model_name)
    # torch.save(model.state_dict(), path_model)
    grad_params_dict = {k: v.to("cpu") for k, v in model.named_parameters()
                        if v.requires_grad == True}
    torch.save(grad_params_dict, path_model)
    print("******model_save_path is {}******".format(path_model))
def prepare_model_for_half_training(model, output_embedding_layer_name="lm_head",
        use_gradient_checkpointing=True, layer_norm_names=["layer_norm"]):
    r"""
    This method wrapps the entire protocol for preparing a model before running a training. This includes:
        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm
        head to fp32

    Args:
        model, (`transformers.PreTrainedModel`):
            The loaded model from `transformers`
    """
    #  ä¸è¦ä½¿ç”¨ model.half(), è¿™æ ·ä¼šå…ˆæˆªå–ç²¾åº¦å†è®­ç»ƒäº†, æœ€åˆdataå°±è¦ä¿æŒhalf
    for name, param in model.named_parameters():
        # freeze base model's layers
        param.requires_grad = False
        # cast layer norm in fp32 for stability for 8bit models
        if param.ndim == 1 and any(layer_norm_name in name for layer_norm_name in layer_norm_names):
            param.data = param.data.to(torch.float32)
        elif output_embedding_layer_name in name:  # lm_headä¹Ÿéœ€è¦æ˜¯tf.float32(æœ€åä¸€å±‚)
            param.data = param.data.to(torch.float32)
        else:
            param.data = param.data.to(torch.half)

    if use_gradient_checkpointing:
        # For backward compatibility
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()
        else:
            def make_inputs_require_grad(module, input, output):
                output.requires_grad_(True)
            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
        # enable gradient checkpointing for memory efficiency
        model.gradient_checkpointing_enable()
    return model
def print_named_parameters(model, use_print_data=True):
    """   æ‰“å°æ¨¡å‹è®­ç»ƒå‚æ•°/æ•°æ®ç±»å‹ä¿¡æ¯   """
    trainable_params = 0
    all_param = 0
    for name, param in model.named_parameters():
        if use_print_data:
            print((name, param.data.dtype, param.requires_grad, param.data))
        else:
            print((name, param.data.dtype, param.requires_grad))
        num_params = param.numel()
        # if using DS Zero 3 and the weights are initialized empty
        if num_params == 0 and hasattr(param, "ds_numel"):
            num_params = param.ds_numel
        all_param += num_params
        if param.requires_grad:
            trainable_params += num_params
    print(f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}")
def generate_prompt_train(data_point, is_logger=False):
    """ ç›´æ¥encodeï¼Œæœ€åå¼ºåˆ¶è¡¥ä¸Š(ä¸èµ°æ–°å¢äº†)ï¼Œ
    ç¨å¾®æœ‰æ‰€ä¸åŒï¼Œæ¨ç†çš„text_1éœ€è¦åŠ ä¸Šassistant\n, ä¸è®­ç»ƒç¨æœ‰ä¸åŒ,
    å¹¶ä¸”ç»ˆæ­¢æ¡ä»¶ä¸º<|im_start|>ã€<|im_end|>ã€<|endoftext|>ä¸‰ä¸ª
    """
    # sorry about the formatting disaster gotta move fast
    # text_1 = f"æŒ‡ä»¤ï¼š\n{data_point.get('instruction', '')}\né—®ï¼š\n{data_point.get('input', '')}\nç­”ï¼š\n" \
    #     if data_point.get('input', '') else f"æŒ‡ä»¤ï¼š\n{data_point.get('instruction', '')}\nç­”ï¼š\n"
    # text_2 = f"{data_point.get('output', '')}"
    system_str = "You are a helpful assistant."
    prompt_system = "<|im_start|>system\n{}<|im_end|>\n".format(system_str)
    text_input = data_point.get("instruction", "") + "\t" + data_point.get("input", "")
    text_out = data_point.get("output", "")
    prompt_text_1 = prompt_system + "<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n"
    prompt_text_2 = "{}<|im_end|><|endoftext|>"
    text_1 = prompt_text_1.format(text_input.strip())
    text_2 = prompt_text_2.format(text_out)
    # end with <|im_end|><|endoftext|>
    x = tokenizer.encode(text_1)
    y = tokenizer.encode(text_2)
    if len(x) + len(y) > (MAX_LENGTH_Q + MAX_LENGTH_A):
        x = x[:MAX_LENGTH_Q]
        y = y[:MAX_LENGTH_A]
    if not x:
        x = [ID_SOP, ID_EOP, ID_BR, ID_SOP]
    x[-3:] = [ID_EOP, ID_BR, ID_SOP]
    if not y:
        y = [ID_EOP, ID_EOS]
    y[-2:] = [ID_EOP, ID_EOS]
    out = {"input_ids": x, "labels": y}
    if is_logger:
        print(text_1)
        print(text_2)
        print(out)
    return out
def generate_prompt(data_point, is_logger=False):
    """ ç›´æ¥encodeï¼Œæœ€åå¼ºåˆ¶è¡¥ä¸Š(ä¸èµ°æ–°å¢äº†)ï¼Œ
    ç¨å¾®æœ‰æ‰€ä¸åŒï¼Œæ¨ç†çš„text_1éœ€è¦åŠ ä¸Šassistant\n, ä¸è®­ç»ƒç¨æœ‰ä¸åŒ,
    å¹¶ä¸”ç»ˆæ­¢æ¡ä»¶ä¸º<|im_start|>ã€<|im_end|>ã€<|endoftext|>ä¸‰ä¸ª
    """
    # sorry about the formatting disaster gotta move fast
    # text_1 = f"æŒ‡ä»¤ï¼š\n{data_point.get('instruction', '')}\né—®ï¼š\n{data_point.get('input', '')}\nç­”ï¼š\n" \
    #     if data_point.get('input', '') else f"æŒ‡ä»¤ï¼š\n{data_point.get('instruction', '')}\nç­”ï¼š\n"
    # text_2 = f"{data_point.get('output', '')}"
    system_str = "You are a helpful assistant."
    prompt_system = "<|im_start|>system\n{}<|im_end|>\n".format(system_str)
    text_input = data_point.get("instruction", "") + "\t" + data_point.get("input", "")
    text_out = data_point.get("output", "")
    prompt_text_1 = prompt_system + "<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n"
    prompt_text_2 = "{}<|im_end|><|endoftext|>"
    text_1 = prompt_text_1.format(text_input.strip())
    text_2 = prompt_text_2.format(text_out)
    # end with <|im_end|><|endoftext|>
    x = tokenizer.encode(text_1)
    y = tokenizer.encode(text_2)
    if len(x) + len(y) > (MAX_LENGTH_Q + MAX_LENGTH_A):
        x = x[:MAX_LENGTH_Q]
        y = y[:MAX_LENGTH_A]
    if not x:
        x = [ID_SOP, ID_EOP, ID_BR, ID_SOP, ID_ASSISTANT, ID_BR]
    x[-5:] = [ID_EOP, ID_BR, ID_SOP, ID_ASSISTANT, ID_BR]
    if not y:
        y = [ID_EOP, ID_EOS]
    y[-2:] = [ID_EOP, ID_EOS]
    out = {"input_ids": x, "labels": y}
    if is_logger:
        print(text_1)
        print(text_2)
        print(out)
    return out

def generate_prompt_offical(data_point, is_logger=False):
    """   å®˜æ–¹ prompt """
    from qwen_sft.models.qwen.qwen_generation_utils import make_context
    system = "You are a helpful assistant."
    query = data_point.get("instruction", "") + "\t" + data_point.get("input", "")
    raw_text, context_tokens = make_context(
                tokenizer,
                query.strip(),
                history=[],
                system=system,
                max_window_size=6144
    )
    out = {"input_ids": context_tokens}
    return out


model = LLMModel.from_pretrained(PATH_MODEL_PRETRAIN)
model = prepare_model_for_half_training(model,
        use_gradient_checkpointing=False,
        output_embedding_layer_name="lm_head",
        layer_norm_names=["ln_1",
                          "ln_2",
                          "ln_f",
                          ],
        )
config = LoraConfig(target_modules=TARGET_MODULES,
                    lora_dropout=LORA_DROPOUT,
                    lora_alpha=LORA_ALPHA,
                    task_type="CAUSAL_LM",
                    bias="none",
                    r=LORA_R,
                    )
model = get_peft_model(model, config)
model = load_model_state(model=model, model_save_dir=MODEL_SAVE_DIR)
if USE_CUDA:
    model = model.cuda()
else:
    model = model.bfloat16()
print_named_parameters(model, use_print_data=True)
# print_named_parameters(model)

tokenizer = LLMTokenizer.from_pretrained(PATH_MODEL_PRETRAIN, add_eos_token=True)
ID_PAD = 151643
ID_EOS = 151643  # endoftext
ID_SOP = 151644  # start
ID_EOP = 151645  # end
ID_BR = 198  # "\n"
ID_ASSISTANT = 77091  # assistantr
tokenizer.pad_token_id = ID_PAD
tokenizer.eos_token_id = ID_EOS
### æ¨ç†ä¸­çš„assistant\nåœ¨é—®å¥ä¸­
# STOP_WORDS_IDS = [[ID_EOS]]
STOP_WORDS_IDS = [[ID_SOP], [ID_EOP], [ID_EOS]]
# tokenizer.padding_side = "right"  # NO attention-mask
tokenizer.padding_side = "left"
print(ID_PAD)
print(ID_EOS)
print(ID_SOP)
print(ID_EOP)
print(ID_BR)
print(ID_ASSISTANT)
# ENDOFTEXT = "<|endoftext|>"
# IMSTART = "<|im_start|>"
# IMEND = "<|im_end|>"


def predict(data_dict):
    """  æ¨ç†  """
    prompt_dict = generate_prompt(data_dict)
    # inputs = tokenizer([text_1], return_tensors="pt", padding=True)
    input_ids = prompt_dict.get("input_ids")
    input_ids = torch.tensor([input_ids], dtype=torch.long)
    if USE_CUDA:
        input_ids = input_ids.cuda()
    generation_config = GenerationConfig(
        # temperature=0.95,
        # top_p=0.75,
        temperature=0.8,
        top_p=0.8,
        top_k=50,
        num_beams=1,
        do_sample=True,
        max_new_tokens=256,
        # penalty_alpha=1.5,
        pad_token_id=ID_PAD,
        eos_token_id=ID_EOS,
    )
    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            stop_words_ids=STOP_WORDS_IDS,
            # stop_words_ids=[[ID_EOS]],
            return_dict_in_generate=True,
            # return_dict_in_generate=True,
            # output_scores=True,
            # max_new_tokens=512,
        )
    s = generation_output.sequences[0]
    output = tokenizer.decode(s)
    print(data_dict)
    print(input_ids)
    print(output)
    # output = output.split("ç­”ï¼š")[-1]
    return output



if __name__ == '__main__':
    data_dict = {"instruction": "è§£é‡Šä¸ºä»€ä¹ˆä¸‹é¢çš„åˆ†æ•°ç­‰äº 1/4",
                 "input": "è§£é‡Šä¸ºä»€ä¹ˆä¸‹é¢çš„åˆ†æ•°ç­‰äº 1/4ï¼Œ4/16",
                 "output": "åˆ†æ•° 4/16 ç­‰äº 1/4ï¼Œå› ä¸ºåˆ†å­å’Œåˆ†æ¯éƒ½å¯ä»¥è¢« 4 æ•´é™¤ã€‚å°†é¡¶éƒ¨å’Œåº•éƒ¨æ•°å­—éƒ½é™¤ä»¥ 4 å¾—åˆ°åˆ†æ•° 1/4ã€‚"
                 }
    res = predict(data_dict)
    print(res)
    while True:
        time_start = time.time()
        history = []
        print("è¯·è¾“å…¥:")
        ques = input()
        print("è¯·ç¨ç­‰...")
        try:
            if ques.strip().upper() == "CLEAR":
                history = []
                print("clear ok")
                continue
            else:
                print("#" * 128)
                ques_dict = {"instruction": ques, "input": "", "output": ""}
                res = predict(ques_dict)
                print(res)
                print("###mid###" * 32)
                response, history = model.chat(tokenizer, ques, history=history)
                print(response)
                print("#" * 128)
        except Exception as e:
            print(str(e))
        print(time.time() - time_start)

"""
python predict.py

### æ‰“å°äº†2æ¬¡å¹¶ä¸chatå¯¹æ¯”
trainable params: 0 || all params: 7731482624 || trainable%: 0.0
151643
151643
151644
151645
{'instruction': 'è§£é‡Šä¸ºä»€ä¹ˆä¸‹é¢çš„åˆ†æ•°ç­‰äº 1/4', 'input': 'è§£é‡Šä¸ºä»€ä¹ˆä¸‹é¢çš„åˆ†æ•°ç­‰äº 1/4ï¼Œ4/16', 'output': 'åˆ†æ•° 4/16 ç­‰äº 1/4ï¼Œå› ä¸ºåˆ†å­å’Œåˆ†æ¯éƒ½å¯ä»¥è¢« 4 æ•´é™¤ã€‚å°†é¡¶éƒ¨å’Œåº•éƒ¨æ•°å­—éƒ½é™¤ä»¥ 4 å¾—åˆ°åˆ†æ•° 1/4ã€‚'}
tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,
         151645,    198, 151644,    872,    198, 104136, 100678, 100431,   9370,
         103190, 107106,    220,     16,     14,     19,    197, 104136, 100678,
         100431,   9370, 103190, 107106,    220,     16,     14,     19,   3837,
             19,     14,     16,     21, 151645,    198, 151644]])
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
è§£é‡Šä¸ºä»€ä¹ˆä¸‹é¢çš„åˆ†æ•°ç­‰äº 1/4    è§£é‡Šä¸ºä»€ä¹ˆä¸‹é¢çš„åˆ†æ•°ç­‰äº 1/4ï¼Œ4/16<|im_end|>
<|im_start|>assistant
å½“æŠŠä¸€ä¸ªåˆ†æ•°çš„åˆ†å­å’Œåˆ†æ¯éƒ½é™¤ä»¥ç›¸åŒçš„æ•°æ—¶ï¼Œåˆ†æ•°å¤§å°ä¸å˜ï¼Œè¿™ä¸ªæ•°å°±å«åšè¿™ä¸ªåˆ†æ•°çš„æœ€å°å…¬å€æ•°ã€‚4/16çš„åˆ†å­å’Œåˆ†æ¯çš„æœ€å¤§å…¬çº¦æ•°æ˜¯4ï¼Œæ‰€ä»¥4/16çš„åˆ†æ•°å¯ä»¥å†™æˆ1/4çš„å½¢å¼ã€‚<|im_end|><|endoftext|>
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
è§£é‡Šä¸ºä»€ä¹ˆä¸‹é¢çš„åˆ†æ•°ç­‰äº 1/4    è§£é‡Šä¸ºä»€ä¹ˆä¸‹é¢çš„åˆ†æ•°ç­‰äº 1/4ï¼Œ4/16<|im_end|>
<|im_start|>assistant
å½“æŠŠä¸€ä¸ªåˆ†æ•°çš„åˆ†å­å’Œåˆ†æ¯éƒ½é™¤ä»¥ç›¸åŒçš„æ•°æ—¶ï¼Œåˆ†æ•°å¤§å°ä¸å˜ï¼Œè¿™ä¸ªæ•°å°±å«åšè¿™ä¸ªåˆ†æ•°çš„æœ€å°å…¬å€æ•°ã€‚4/16çš„åˆ†å­å’Œåˆ†æ¯çš„æœ€å¤§å…¬çº¦æ•°æ˜¯4ï¼Œæ‰€ä»¥4/16çš„åˆ†æ•°å¯ä»¥å†™æˆ1/4çš„å½¢å¼ã€‚<|im_end|><|endoftext|>
è¯·è¾“å…¥:
ç±»å‹#ä¸Šè¡£*é£æ ¼#è¡—å¤´*å›¾æ¡ˆ#åˆ›æ„*è¡£æ ·å¼#å«è¡£
è¯·ç¨ç­‰...
################################################################################################################################
{'instruction': 'ç±»å‹#ä¸Šè¡£*é£æ ¼#è¡—å¤´*å›¾æ¡ˆ#åˆ›æ„*è¡£æ ·å¼#å«è¡£', 'input': '', 'output': ''}
tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,
         151645,    198, 151644,    872,    198,  31905,      2,  17447,  99741,
              9, 104040,      2, 108094,      9, 108108,      2, 102343,      9,
          99741, 112453,      2,  99417,  99741, 151645,    198, 151644]])
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
ç±»å‹#ä¸Šè¡£*é£æ ¼#è¡—å¤´*å›¾æ¡ˆ#åˆ›æ„*è¡£æ ·å¼#å«è¡£<|im_end|>
<|im_start|>assistant
å«è¡£æ˜¯å¾ˆå¤šæ—¶å°šæ½®äººå–œçˆ±çš„å•å“ï¼Œè€Œè¿™æ¬¾å«è¡£æœ€å¤§çš„äº®ç‚¹å°±åœ¨äºè¡£èº«çš„logoè®¾è®¡ï¼Œä¸ä»…èµ·åˆ°äº†è£…é¥°çš„ä½œç”¨ï¼Œè€Œä¸”å½°æ˜¾å‡ºå“ç‰Œçš„ç‹¬ç‰¹åˆ›æ„ã€‚å†åŠ ä¸Šoversizeçš„ç‰ˆå‹ï¼Œè¡—å¤´èŒƒå„¿åè¶³ã€‚<|im_end|><|endoftext|>
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
ç±»å‹#ä¸Šè¡£*é£æ ¼#è¡—å¤´*å›¾æ¡ˆ#åˆ›æ„*è¡£æ ·å¼#å«è¡£<|im_end|>
<|im_start|>assistant
å«è¡£æ˜¯å¾ˆå¤šæ—¶å°šæ½®äººå–œçˆ±çš„å•å“ï¼Œè€Œè¿™æ¬¾å«è¡£æœ€å¤§çš„äº®ç‚¹å°±åœ¨äºè¡£èº«çš„logoè®¾è®¡ï¼Œä¸ä»…èµ·åˆ°äº†è£…é¥°çš„ä½œç”¨ï¼Œè€Œä¸”å½°æ˜¾å‡ºå“ç‰Œçš„ç‹¬ç‰¹åˆ›æ„ã€‚å†åŠ ä¸Šoversizeçš„ç‰ˆå‹ï¼Œè¡—å¤´èŒƒå„¿åè¶³ã€‚<|im_end|><|endoftext|>
###mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid###
################################################################################################################################
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
ç±»å‹#ä¸Šè¡£*é£æ ¼#è¡—å¤´*å›¾æ¡ˆ#åˆ›æ„*è¡£æ ·å¼#å«è¡£<|im_end|>
<|im_start|>assistant

[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 31905, 2, 17447, 99741, 9, 104040, 2, 108094, 9, 108108, 2, 102343, 9, 99741, 112453, 2, 99417, 99741, 151645, 198, 151644, 77091, 198]
tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,
         151645,    198, 151644,    872,    198,  31905,      2,  17447,  99741,
              9, 104040,      2, 108094,      9, 108108,      2, 102343,      9,
          99741, 112453,      2,  99417,  99741, 151645,    198, 151644,  77091,
            198,  99417,  99741,  18493, 109385, 105419, 107849, 103215,   3837,
         100405,  99518,  99271, 100095,   3837, 106284, 114302, 114658,   9370,
         116679,  34187,   1773, 101469,  99417,  99741, 101910,  99436,  77540,
          14224, 105171,   3837, 100629, 106647,   9370, 102343,  98650,   3837,
          91572,  99518,  99278,  27442, 102382, 102382,   9370, 108094,  99208,
           3837, 100231, 108094,  99208,   9370, 109455,   1773, 151645, 151643]])
å«è¡£åœ¨æ˜¥ç§‹å­£èŠ‚æ ¼å¤–å—æ¬¢è¿ï¼Œç®€å•åˆç™¾æ­ï¼Œå¯ä»¥è¯´æ˜¯æ—¶é«¦å•å“çš„æ ‡é…äº†ã€‚è¿™æ¬¾å«è¡£é‡‡ç”¨å‡ä¸¤ä»¶çš„è®¾è®¡ï¼Œå…·æœ‰å¾ˆå¼ºçš„åˆ›æ„æ„Ÿï¼ŒåŒæ—¶åˆå¸¦ç‚¹é…·é…·çš„è¡—å¤´é£ï¼Œé€‚åˆè¡—å¤´é£çš„å¦¹å­ã€‚
################################################################################################################################
167.13920497894287
è¯·è¾“å…¥:
ç±»å‹#è£¤*æè´¨#ç‰›ä»”å¸ƒ*é£æ ¼#æ½®*è£¤å‹#å“ˆä¼¦è£¤*è£¤è…°å‹#æ¾ç´§è…°*è£¤å£#å°è„š
è¯·ç¨ç­‰...
################################################################################################################################
{'instruction': 'ç±»å‹#è£¤*æè´¨#ç‰›ä»”å¸ƒ*é£æ ¼#æ½®*è£¤å‹#å“ˆä¼¦è£¤*è£¤è…°å‹#æ¾ç´§è…°*è£¤å£#å°è„š', 'input': '', 'output': ''}
tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,
         151645,    198, 151644,    872,    198,  31905,      2, 102693,      9,
         106149,      2, 100664, 102437,  51827,      9, 104040,      2, 100227,
              9, 102693,  24300,      2,  98671, 100794, 102693,      9, 102693,
         102113,  24300,      2, 100180,  99378, 102113,      9, 102693,  39426,
              2,  30709, 100037, 151645,    198, 151644]])
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
ç±»å‹#è£¤*æè´¨#ç‰›ä»”å¸ƒ*é£æ ¼#æ½®*è£¤å‹#å“ˆä¼¦è£¤*è£¤è…°å‹#æ¾ç´§è…°*è£¤å£#å°è„š<|im_end|>
<|im_start|>assistant
è¿™æ¬¾ç‰›ä»”è£¤é‡‡ç”¨çš„æ˜¯å“ˆä¼¦è£¤çš„ç‰ˆå‹ï¼Œå…·æœ‰ğŸš¹çš„æ½®æµæ°”æ¯ï¼Œæ—¶å°šçš„å“ˆä¼¦è£¤ç‰ˆå‹ï¼Œèƒ½å¤Ÿå¾ˆå¥½çš„ä¿®é¥°è…¿éƒ¨çš„æ›²çº¿ï¼Œä½¿å¾—è…¿éƒ¨æ›´åŠ çš„ä¿®é•¿ã€‚å°è„šçš„è£¤è„šè®¾è®¡ï¼Œèƒ½å¤Ÿå¾ˆå¥½çš„æ‹‰é•¿è…¿éƒ¨çš„æ›²çº¿ï¼ŒåŒæ—¶åˆå…·æœ‰å¾ˆå¥½çš„ä¿æš–æ€§ï¼Œèƒ½å¤Ÿå¾ˆå¥½çš„æŠµå¾¡å¯’é£çš„ä¾µè¢­ã€‚æ¾ç´§çš„è…°å¸¦è®¾è®¡ï¼Œæ›´åŠ çš„æ–¹ä¾¿Touchï¼Œç©¿è„±æ›´åŠ çš„æ–¹ä¾¿ã€‚<|im_end|><|endoftext|>
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
ç±»å‹#è£¤*æè´¨#ç‰›ä»”å¸ƒ*é£æ ¼#æ½®*è£¤å‹#å“ˆä¼¦è£¤*è£¤è…°å‹#æ¾ç´§è…°*è£¤å£#å°è„š<|im_end|>
<|im_start|>assistant
è¿™æ¬¾ç‰›ä»”è£¤é‡‡ç”¨çš„æ˜¯å“ˆä¼¦è£¤çš„ç‰ˆå‹ï¼Œå…·æœ‰ğŸš¹çš„æ½®æµæ°”æ¯ï¼Œæ—¶å°šçš„å“ˆä¼¦è£¤ç‰ˆå‹ï¼Œèƒ½å¤Ÿå¾ˆå¥½çš„ä¿®é¥°è…¿éƒ¨çš„æ›²çº¿ï¼Œä½¿å¾—è…¿éƒ¨æ›´åŠ çš„ä¿®é•¿ã€‚å°è„šçš„è£¤è„šè®¾è®¡ï¼Œèƒ½å¤Ÿå¾ˆå¥½çš„æ‹‰é•¿è…¿éƒ¨çš„æ›²çº¿ï¼ŒåŒæ—¶åˆå…·æœ‰å¾ˆå¥½çš„ä¿æš–æ€§ï¼Œèƒ½å¤Ÿå¾ˆå¥½çš„æŠµå¾¡å¯’é£çš„ä¾µè¢­ã€‚æ¾ç´§çš„è…°å¸¦è®¾è®¡ï¼Œæ›´åŠ çš„æ–¹ä¾¿Touchï¼Œç©¿è„±æ›´åŠ çš„æ–¹ä¾¿ã€‚<|im_end|><|endoftext|>
###mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid######mid###
################################################################################################################################
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
ç±»å‹#è£¤*æè´¨#ç‰›ä»”å¸ƒ*é£æ ¼#æ½®*è£¤å‹#å“ˆä¼¦è£¤*è£¤è…°å‹#æ¾ç´§è…°*è£¤å£#å°è„š<|im_end|>
<|im_start|>assistant

[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 31905, 2, 102693, 9, 106149, 2, 100664, 102437, 51827, 9, 104040, 2, 100227, 9, 102693, 24300, 2, 98671, 100794, 102693, 9, 102693, 102113, 24300, 2, 100180, 99378, 102113, 9, 102693, 39426, 2, 30709, 100037, 151645, 198, 151644, 77091, 198]
tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,
         151645,    198, 151644,    872,    198,  31905,      2, 102693,      9,
         106149,      2, 100664, 102437,  51827,      9, 104040,      2, 100227,
              9, 102693,  24300,      2,  98671, 100794, 102693,      9, 102693,
         102113,  24300,      2, 100180,  99378, 102113,      9, 102693,  39426,
              2,  30709, 100037, 151645,    198, 151644,  77091,    198, 100664,
         102437, 102693, 107359,  98671, 100794,  14618,     27,  31883,   9231,
          40301,  24300, 105171,   3837,  32664, 117656,  18830, 109957, 114999,
         100154,   1773, 104392, 117656,   9370, 101538, 146937, 101538,  99338,
          68536,  23081,  45861,   3837,  91572,  99518,  16530,  99580, 121922,
         101561,   1773, 102113,  39426,  44290, 107359, 100180,  99378,   9370,
         100349, 100526,  62963,  39426,  70500,   3837, 105611, 102476,  99518,
          99580, 102647,   1773,  99739,  99580, 104070,   9370, 106294,  98650,
           1773, 151645, 151643]])
ç‰›ä»”è£¤é‡‡ç”¨äº†å“ˆä¼¦(.<UNK>)ç‰ˆå‹çš„è®¾è®¡ï¼Œå¯¹è…¿éƒ¨æœ‰è¾ƒå¥½çš„ä¿®é¥°ä½œç”¨ã€‚æ˜¾å¾—è…¿éƒ¨çš„çº¤á½–çº¤ç»†è€Œä¿®é•¿ï¼ŒåŒæ—¶åˆä¸æ˜¾è‡ƒè‚¿ã€‚è…°å£å¤„é‡‡ç”¨äº†æ¾ç´§çš„ç½—çº¹æŸå£è®¾è®¡ï¼Œç©¿ç€èˆ’é€‚åˆæ˜¾ä¸ªæ€§ã€‚å°½æ˜¾æ—¶å°šçš„æ½®æµæ„Ÿã€‚
################################################################################################################################
103.0037031173706
è¯·è¾“å…¥:
"""