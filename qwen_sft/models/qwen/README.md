---
language:
- zh
- en
tags:
- qwen
pipeline_tag: text-generation
inference: false
---

# Qwen-7B-Chat

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-7B <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤– </a> | <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>&nbsp ï½œ Qwen-7B-Chat <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤– </a>| <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>&nbsp ï½œ &nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md">Report</a>
</p>
<br>

## ä»‹ç»ï¼ˆIntroductionï¼‰

**é€šä¹‰åƒé—®-7Bï¼ˆQwen-7Bï¼‰**æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—çš„70äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Qwen-7Bæ˜¯åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚åŒæ—¶ï¼Œåœ¨Qwen-7Bçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåŠ©æ‰‹Qwen-7B-Chatã€‚æœ¬ä»“åº“ä¸ºQwen-7B-Chatçš„ä»“åº“ã€‚

å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºé€šä¹‰åƒé—®-7Bå¼€æºæ¨¡å‹çš„ç»†èŠ‚ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨å‚é˜…[Githubä»£ç åº“](https://github.com/QwenLM/Qwen-7B)ã€‚

**Qwen-7B** is the 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-7B`is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-7B, we release Qwen-7B-Chat, a large-model-based AI assistant, which is trained with alignment techniques. This repository is the one for Qwen-7B-Chat.

For more details about the open-source model of Qwen-7B, please refer to the [Github](https://github.com/QwenLM/Qwen-7B) code repository.

## è¦æ±‚ï¼ˆRequirementsï¼‰

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
* python 3.8 and above
* pytorch 1.12 and above, 2.0 and above are recommended
* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)

## ä¾èµ–é¡¹ï¼ˆDependencyï¼‰

è¿è¡ŒQwen-7B-Chatï¼Œè¯·ç¡®ä¿æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œå†æ‰§è¡Œä»¥ä¸‹pipå‘½ä»¤å®‰è£…ä¾èµ–åº“

To run Qwen-7B-Chat, please make sure you meet the above requirements, and then execute the following pip commands to install the dependent libraries.

```bash
pip install transformers==4.31.0 accelerate tiktoken einops
```

å¦å¤–ï¼Œæ¨èå®‰è£…`flash-attention`åº“ï¼Œä»¥å®ç°æ›´é«˜çš„æ•ˆç‡å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ã€‚

In addition, it is recommended to install the `flash-attention` library for higher efficiency and lower memory usage.

```bash
git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚
# Below are optional. Installing them might be slow.
pip install csrc/layer_norm
pip install csrc/rotary
```

## å¿«é€Ÿä½¿ç”¨ï¼ˆQuickstartï¼‰

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨Qwen-7B-Chatæ¨¡å‹ï¼Œè¿›è¡Œå¤šè½®å¯¹è¯äº¤äº’çš„æ ·ä¾‹ï¼š

We show an example of multi-turn interaction with Qwen-7B-Chat in the following code:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Note: The default behavior now has injection attack prevention off.
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()

# Specify hyperparameters for generation
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

# ç¬¬ä¸€è½®å¯¹è¯ 1st dialogue turn
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# ç¬¬äºŒè½®å¯¹è¯ 2nd dialogue turn
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history) 
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# ç¬¬ä¸‰è½®å¯¹è¯ 3rd dialogue turn
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

å…³äºæ›´å¤šçš„ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[Github repo](https://github.com/QwenLM/Qwen-7B)è·å–æ›´å¤šä¿¡æ¯ã€‚

For more information, please refer to our [Github repo](https://github.com/QwenLM/Qwen-7B) for more information.

## Tokenizer

> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚

åŸºäºtiktokençš„åˆ†è¯å™¨æœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepieceåˆ†è¯å™¨ã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://github.com/QwenLM/Qwen-7B/blob/main/tokenization_note_zh.md)ã€‚

Our tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](https://github.com/QwenLM/Qwen-7B/blob/main/tokenization_note.md).

## æ¨¡å‹ç»†èŠ‚ï¼ˆModelï¼‰

ä¸Qwen-7Bé¢„è®­ç»ƒæ¨¡å‹ç›¸åŒï¼ŒQwen-7B-Chatæ¨¡å‹è§„æ¨¡åŸºæœ¬æƒ…å†µå¦‚ä¸‹æ‰€ç¤º

The details of the model architecture of Qwen-7B-Chat are listed as follows

| Hyperparameter | Value |
|:------|:------|
| n_layers | 32 |
| n_heads | 32 |
| d_model | 4096 |
| vocab size | 151851 |
| sequence length | 2048 |

åœ¨ä½ç½®ç¼–ç ã€FFNæ¿€æ´»å‡½æ•°å’Œnormalizationçš„å®ç°æ–¹å¼ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿé‡‡ç”¨äº†ç›®å‰æœ€æµè¡Œçš„åšæ³•ï¼Œ
å³RoPEç›¸å¯¹ä½ç½®ç¼–ç ã€SwiGLUæ¿€æ´»å‡½æ•°ã€RMSNormï¼ˆå¯é€‰å®‰è£…flash-attentionåŠ é€Ÿï¼‰ã€‚

åœ¨åˆ†è¯å™¨æ–¹é¢ï¼Œç›¸æ¯”ç›®å‰ä¸»æµå¼€æºæ¨¡å‹ä»¥ä¸­è‹±è¯è¡¨ä¸ºä¸»ï¼ŒQwen-7B-Chatä½¿ç”¨äº†çº¦15ä¸‡tokenå¤§å°çš„è¯è¡¨ã€‚
è¯¥è¯è¡¨åœ¨GPT-4ä½¿ç”¨çš„BPEè¯è¡¨`cl100k_base`åŸºç¡€ä¸Šï¼Œå¯¹ä¸­æ–‡ã€å¤šè¯­è¨€è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨å¯¹ä¸­ã€è‹±ã€ä»£ç æ•°æ®çš„é«˜æ•ˆç¼–è§£ç çš„åŸºç¡€ä¸Šï¼Œå¯¹éƒ¨åˆ†å¤šè¯­è¨€æ›´åŠ å‹å¥½ï¼Œæ–¹ä¾¿ç”¨æˆ·åœ¨ä¸æ‰©å±•è¯è¡¨çš„æƒ…å†µä¸‹å¯¹éƒ¨åˆ†è¯­ç§è¿›è¡Œèƒ½åŠ›å¢å¼ºã€‚
è¯è¡¨å¯¹æ•°å­—æŒ‰å•ä¸ªæ•°å­—ä½åˆ‡åˆ†ã€‚è°ƒç”¨è¾ƒä¸ºé«˜æ•ˆçš„[tiktokenåˆ†è¯åº“](https://github.com/openai/tiktoken)è¿›è¡Œåˆ†è¯ã€‚

For position encoding, FFN activation function, and normalization calculation methods, we adopt the prevalent practices, i.e., RoPE relative position encoding, SwiGLU for activation function, and RMSNorm for normalization (optional installation of flash-attention for acceleration).

For tokenization, compared to the current mainstream open-source models based on Chinese and English vocabularies, Qwen-7B-Chat uses a vocabulary of over 150K tokens.
It first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary.
It segments numbers by single digit, and calls the [tiktoken](https://github.com/openai/tiktoken) tokenizer library for efficient tokenization.

## è¯„æµ‹æ•ˆæœï¼ˆEvaluationï¼‰

å¯¹äºQwen-7B-Chatæ¨¡å‹ï¼Œæˆ‘ä»¬åŒæ ·è¯„æµ‹äº†å¸¸è§„çš„ä¸­æ–‡ç†è§£ï¼ˆC-Evalï¼‰ã€è‹±æ–‡ç†è§£ï¼ˆMMLUï¼‰ã€ä»£ç ï¼ˆHumanEvalï¼‰å’Œæ•°å­¦ï¼ˆGSM8Kï¼‰ç­‰æƒå¨ä»»åŠ¡ï¼ŒåŒæ—¶åŒ…å«äº†é•¿åºåˆ—ä»»åŠ¡çš„è¯„æµ‹ç»“æœã€‚ç”±äºQwen-7B-Chatæ¨¡å‹ç»è¿‡å¯¹é½åï¼Œæ¿€å‘äº†è¾ƒå¼ºçš„å¤–éƒ¨ç³»ç»Ÿè°ƒç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢çš„è¯„æµ‹ã€‚

æç¤ºï¼šç”±äºç¡¬ä»¶å’Œæ¡†æ¶é€ æˆçš„èˆå…¥è¯¯å·®ï¼Œå¤ç°ç»“æœå¦‚æœ‰æ³¢åŠ¨å±äºæ­£å¸¸ç°è±¡ã€‚

For Qwen-7B-Chat, we also evaluate the model on C-Eval, MMLU, HumanEval, GSM8K, etc., as well as the benchmark evaluation for long-context understanding, and tool usage.

Note: Due to rounding errors caused by hardware and framework, differences in reproduced results are possible.

### ä¸­æ–‡è¯„æµ‹ï¼ˆChinese Evaluationï¼‰

#### C-Eval

åœ¨[C-Eval](https://arxiv.org/abs/2305.08322)éªŒè¯é›†ä¸Šï¼Œæˆ‘ä»¬è¯„ä»·äº†Qwen-7B-Chatæ¨¡å‹çš„zero-shotå‡†ç¡®ç‡

We demonstrate the zero-shot accuracy of Qwen-7B-Chat on C-Eval validation set

| Model | Avg. Acc. |
|:--------------|:------:|
| LLaMA2-7B-Chat | 31.9 |
| LLaMA2-13B-Chat | 40.6 |
| Chinese-Alpaca-2-7B | 41.3 |
| Chinese-Alpaca-Plus-13B | 43.3 |
| Baichuan-13B-Chat | 50.4 |
| ChatGLM2-6B-Chat | 50.7 |
| InternLM-7B-Chat | 53.2 |
| **Qwen-7B-Chat** | **54.2** |

C-Evalæµ‹è¯•é›†ä¸Šï¼ŒQwen-7B-Chatæ¨¡å‹çš„zero-shotå‡†ç¡®ç‡ç»“æœå¦‚ä¸‹ï¼š

The zero-shot accuracy of Qwen-7B-Chat on C-Eval testing set is provided below:

| Model | Avg. | STEM | Social Sciences | Humanities | Others |
|:--------------|:------:|:------:|:------:|:------:|:------:|
| Chinese-Alpaca-Plus-13B | 41.5 | 36.6 | 49.7 | 43.1 | 41.2 |
| Chinese-Alpaca-2-7B | 40.3 | - | - | - | - |
| ChatGLM2-6B-Chat | 50.1 | 46.4 | 60.4 | 50.6 | 46.9 |
| Baichuan-13B-Chat | 51.5 | 43.7 | 64.6 | 56.2 | 49.2 |
| **Qwen-7B-Chat** | **54.6** | 47.8 | 67.6 | 59.3 | 50.6 |

åœ¨7Bè§„æ¨¡æ¨¡å‹ä¸Šï¼Œç»è¿‡äººç±»æŒ‡ä»¤å¯¹é½çš„Qwen-7B-Chatæ¨¡å‹ï¼Œå‡†ç¡®ç‡åœ¨åŒç±»ç›¸è¿‘è§„æ¨¡æ¨¡å‹ä¸­ä»ç„¶å¤„äºå‰åˆ—ã€‚

Compared with other pretrained models with comparable model size, the human-aligned Qwen-7B-Chat performs well in C-Eval accuracy.

### è‹±æ–‡è¯„æµ‹ï¼ˆEnglish Evaluationï¼‰

#### MMLU

[MMLU](https://arxiv.org/abs/2009.03300)è¯„æµ‹é›†ä¸Šï¼ŒQwen-7B-Chatæ¨¡å‹çš„zero-shotå‡†ç¡®ç‡å¦‚ä¸‹ï¼Œæ•ˆæœåŒæ ·åœ¨åŒç±»å¯¹é½æ¨¡å‹ä¸­åŒæ ·è¡¨ç°è¾ƒä¼˜ã€‚

The zero-shot accuracy of Qwen-7B-Chat on MMLU is provided below.
The performance of Qwen-7B-Chat still on the top between other human-aligned models with comparable size.

| Model | Avg. Acc. |
|:--------------|:------:|
| ChatGLM2-6B-Chat | 45.5 |
| LLaMA2-7B-Chat | 47.0 |
| InternLM-7B-Chat | 50.8 |
| Baichuan-13B-Chat | 52.1 |
| ChatGLM2-12B-Chat | 52.1 |
| **Qwen-7B-Chat** | **53.9** |

### ä»£ç è¯„æµ‹ï¼ˆCoding Evaluationï¼‰

Qwen-7B-Chatåœ¨[HumanEval](https://github.com/openai/human-eval)çš„zero-shot Pass@1æ•ˆæœå¦‚ä¸‹

The zero-shot Pass@1 of Qwen-7B-Chat on [HumanEval](https://github.com/openai/human-eval) is demonstrated below

| Model | Pass@1 |
|:--------------|:------:|
| LLaMA2-7B-Chat | 12.2 |
| InternLM-7B-Chat | 14.0 |
| Baichuan-13B-Chat | 16.5 |
| LLaMA2-13B-Chat | 18.9 |
| **Qwen-7B-Chat** | **24.4** |

### æ•°å­¦è¯„æµ‹

åœ¨è¯„æµ‹æ•°å­¦èƒ½åŠ›çš„[GSM8K](https://github.com/openai/grade-school-math)ä¸Šï¼ŒQwen-7B-Chatçš„å‡†ç¡®ç‡ç»“æœå¦‚ä¸‹

The accuracy of Qwen-7B-Chat on GSM8K is shown below

| Model | Zero-shot Acc. | 4-shot Acc. |
|:--------------|:------:|:------:|
| ChatGLM2-6B-Chat |  -  | 28.0 |
| LLaMA2-7B-Chat | 20.4 | 28.2 |
| LLaMA2-13B-Chat | 29.4 | 36.7 |
| InternLM-7B-Chat | 32.6 | 34.5 |
| Baichuan-13B-Chat | -  | 36.3 |
| ChatGLM2-12B-Chat | -  | 38.1 |
| **Qwen-7B-Chat** | **41.1** | **43.5** |

### é•¿åºåˆ—è¯„æµ‹ï¼ˆLong-Context Understandingï¼‰

é€šè¿‡NTKæ’å€¼ï¼ŒLogNæ³¨æ„åŠ›ç¼©æ”¾å¯ä»¥æ‰©å±•Qwen-7B-Chatçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚åœ¨é•¿æ–‡æœ¬æ‘˜è¦æ•°æ®é›†[VCSUM](https://arxiv.org/abs/2305.05280)ä¸Šï¼ˆæ–‡æœ¬å¹³å‡é•¿åº¦åœ¨15Kå·¦å³ï¼‰ï¼ŒQwen-7B-Chatçš„Rouge-Lç»“æœå¦‚ä¸‹ï¼š

**(è‹¥è¦å¯ç”¨è¿™äº›æŠ€å·§ï¼Œè¯·å°†config.jsoné‡Œçš„`use_dynamc_ntk`å’Œ`use_logn_attn`è®¾ç½®ä¸ºtrue)**

We introduce NTK-aware interpolation, LogN attention scaling to extend the context length of Qwen-7B-Chat. The Rouge-L results of Qwen-7B-Chat on long-text summarization dataset [VCSUM](https://arxiv.org/abs/2305.05280) (The average length of this dataset is around 15K) are shown below:

**(To use these tricks, please set `use_dynamic_ntk` and `use_long_attn` to true in config.json.)**

| Model | VCSUM (zh) |
|:----------------|:-------:|
| GPT-3.5-Turbo-16k | 16.0 |
| LLama2-7B-Chat	|	0.2 |
| InternLM-7B-Chat | 13.0 |
| ChatGLM2-6B-Chat	| 16.3 |
| **Qwen-7B-Chat** | **16.6** |

### å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è¯„æµ‹ï¼ˆTool Usageï¼‰

#### ReAct Prompting

åƒé—®æ”¯æŒé€šè¿‡ [ReAct Prompting](https://arxiv.org/abs/2210.03629) è°ƒç”¨æ’ä»¶/å·¥å…·/APIã€‚ReAct ä¹Ÿæ˜¯ [LangChain](https://python.langchain.com/) æ¡†æ¶é‡‡ç”¨çš„ä¸»è¦æ–¹å¼ä¹‹ä¸€ã€‚åœ¨å³å°†å¼€æºçš„ã€ç”¨äºè¯„ä¼°å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è‡ªå»ºè¯„æµ‹åŸºå‡†ä¸Šï¼Œåƒé—®çš„è¡¨ç°å¦‚ä¸‹ï¼š

Qwen-7B-Chat supports calling plugins/tools/APIs through [ReAct Prompting](https://arxiv.org/abs/2210.03629). ReAct is also one of the main approaches used by the [LangChain](https://python.langchain.com/) framework. In the soon-to-be-released evaluation benchmark for assessing tool usage capabilities, Qwen-7B-Chat's performance is as follows:

| Model            | Tool Selection (Acc.â†‘) | Tool Input (Rouge-Lâ†‘) | False Positive Errorâ†“ |
|:-----------------|:----------------------:|:---------------------:|:---------------------:|
| GPT-4            | 95%                    | **0.90**              | 15%                   |
| GPT-3.5          | 85%                    | 0.88                  | 75%                   |
| **Qwen-7B-Chat** | **99%**                | 0.89                  | **8.5%**              |

> è¯„æµ‹åŸºå‡†ä¸­å‡ºç°çš„æ’ä»¶å‡æ²¡æœ‰å‡ºç°åœ¨åƒé—®çš„è®­ç»ƒé›†ä¸­ã€‚è¯¥åŸºå‡†è¯„ä¼°äº†æ¨¡å‹åœ¨å¤šä¸ªå€™é€‰æ’ä»¶ä¸­é€‰æ‹©æ­£ç¡®æ’ä»¶çš„å‡†ç¡®ç‡ã€ä¼ å…¥æ’ä»¶çš„å‚æ•°çš„åˆç†æ€§ã€ä»¥åŠå‡é˜³ç‡ã€‚å‡é˜³ç‡ï¼ˆFalse Positiveï¼‰å®šä¹‰ï¼šåœ¨å¤„ç†ä¸è¯¥è°ƒç”¨æ’ä»¶çš„è¯·æ±‚æ—¶ï¼Œé”™è¯¯åœ°è°ƒç”¨äº†æ’ä»¶ã€‚

> The plugins that appear in the evaluation set do not appear in the training set of Qwen-7B-Chat. This benchmark evaluates the accuracy of the model in selecting the correct plugin from multiple candidate plugins, the rationality of the parameters passed into the plugin, and the false positive rate. False Positive: Incorrectly invoking a plugin when it should not have been called when responding to a query.

å…³äº ReAct Prompting çš„ prompt æ€ä¹ˆå†™ã€æ€ä¹ˆä½¿ç”¨ï¼Œè¯·å‚è€ƒ [ReAct æ ·ä¾‹è¯´æ˜](examples/react_prompt.md)ã€‚ä½¿ç”¨å·¥å…·èƒ½ä½¿æ¨¡å‹æ›´å¥½åœ°å®Œæˆä»»åŠ¡ã€‚åŸºäºåƒé—®çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬èƒ½å®ç°ä¸‹å›¾æ‰€å±•ç¤ºçš„æ•ˆæœï¼š

For how to write and use prompts for ReAct Prompting, please refer to [the ReAct examples](examples/react_prompt.md). The use of tools can enable the model to better perform tasks, as shown in the following figures:

![](assets/react_showcase_001.png)
![](assets/react_showcase_002.png)

#### Huggingface Agent

åƒé—®è¿˜å…·å¤‡ä½œä¸º [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents) çš„èƒ½åŠ›ã€‚å®ƒåœ¨ Huggingface æä¾›çš„runæ¨¡å¼è¯„æµ‹åŸºå‡†ä¸Šçš„è¡¨ç°å¦‚ä¸‹ï¼š

Qwen-7B-Chat also has the capability to be used as a [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents). Its performance on the run-mode benchmark provided by HuggingFace is as follows:

| Model | Tool Selectionâ†‘ | Tool Usedâ†‘ | Codeâ†‘ |
|:-|:-:|:-:|:-:|
|GPT-4 | **100** | **100** | **97.41** |
|GPT-3.5 | 95.37 | 96.30 | 87.04 |
|StarCoder-15.5B | 87.04 | 87.96 | 68.89 |
| **Qwen-7B** | 90.74 | 92.59 | 74.07 |

## é‡åŒ–ï¼ˆQuantizationï¼‰

å¦‚å¸Œæœ›ä½¿ç”¨æ›´ä½ç²¾åº¦çš„é‡åŒ–æ¨¡å‹ï¼Œå¦‚4æ¯”ç‰¹å’Œ8æ¯”ç‰¹çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æä¾›äº†ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•å¿«é€Ÿä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº†`bitsandbytes`ã€‚è¯·æ³¨æ„ï¼Œ`bitsandbytes`çš„å®‰è£…è¦æ±‚æ˜¯ï¼š

We provide examples to show how to load models in `NF4` and `Int8`. For starters, make sure you have implemented `bitsandbytes`. Note that the requirements for `bitsandbytes` are:

```
**Requirements** Python >=3.8. Linux distribution (Ubuntu, MacOS, etc.) + CUDA > 10.0.
```

Windowsç”¨æˆ·éœ€å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„`bitsandbytes`ï¼Œå¯é€‰é¡¹åŒ…æ‹¬[bitsandbytes-windows-webui](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels)ã€‚

Windows users should find another option, which might be [bitsandbytes-windows-webui](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels).

ä½ åªéœ€è¦åœ¨`AutoModelForCausalLM.from_pretrained`ä¸­æ·»åŠ ä½ çš„é‡åŒ–é…ç½®ï¼Œå³å¯ä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼š

Then you only need to add your quantization configuration to `AutoModelForCausalLM.from_pretrained`. See the example below:

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# quantization configuration for NF4 (4 bits)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.bfloat16
)

# quantization configuration for Int8 (8 bits)
quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
    device_map="cuda:0",
    quantization_config=quantization_config,
    max_memory=max_memory,
    trust_remote_code=True,
).eval()
```

ä¸Šè¿°æ–¹æ³•å¯ä»¥è®©æˆ‘ä»¬å°†æ¨¡å‹é‡åŒ–æˆ`NF4`å’Œ`Int8`ç²¾åº¦çš„æ¨¡å‹è¿›è¡Œè¯»å–ï¼Œå¸®åŠ©æˆ‘ä»¬èŠ‚çœæ˜¾å­˜å¼€é”€ã€‚æˆ‘ä»¬ä¹Ÿæä¾›äº†ç›¸å…³æ€§èƒ½æ•°æ®ã€‚æˆ‘ä»¬å‘ç°å°½ç®¡æ¨¡å‹åœ¨æ•ˆæœä¸Šå­˜åœ¨æŸå¤±ï¼Œä½†æ¨¡å‹çš„æ˜¾å­˜å¼€é”€å¤§å¹…é™ä½ã€‚

With this method, it is available to load Qwen-7B-Chat in `NF4`and `Int8`, which saves you memory usage. We provide related statistics of model performance below. We find that the quantization downgrades the effectiveness slightly but significantly increases inference efficiency and reduces memory costs.

| Precision | MMLU | Memory |
| :---------| :-------: | :-----: |
|   BF16   |  56.7 |   16.2G |
|   Int8   |  52.8 |   10.1G |
|    NF4    |  48.9 |    7.4G |

## ä½¿ç”¨åè®®ï¼ˆLicense Agreementï¼‰

æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¹¶æ”¯æŒå•†ç”¨ã€‚è¯·æŸ¥çœ‹LICENSEäº†è§£å…·ä½“çš„å¼€æºåè®®ç»†èŠ‚ã€‚

Our code and checkpoints are open to research purpose, and they are allowed for commercial purposes. Check [LICENSE](LICENSE) for more details about the license.

## è”ç³»æˆ‘ä»¬ï¼ˆContact Usï¼‰

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

If you are interested to leave a message to either our research team or product team, feel free to send an email to qianwen_opensource@alibabacloud.com.

